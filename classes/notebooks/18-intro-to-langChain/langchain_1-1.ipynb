{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkNYjLoSrk-X"
      },
      "source": [
        "# Section 1. Introduction to LangChain\n",
        "\n",
        "## Introduction\n",
        "\n",
        "LangChain is an open-source framework that simplifies the development of applications powered by language models (LLMs). In a rapidly evolving landscape of language models, LangChain stands out by offering a more accessible and flexible way to integrate these technologies into a wide range of applications. This enables the creation of smarter and more personalized solutions, tailored to the specific needs of each context.\n",
        "\n",
        "LangChain applications span various areas:\n",
        "* Intelligent and conversational chatbots;\n",
        "* Creative multimedia content generation;\n",
        "* Data analysis for extracting valuable insights;\n",
        "* Virtual assistants capable of automating complex tasks;\n",
        "* Accurate and informative question-and-answer systems."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 About API\n",
        "\n",
        "Unlike direct interaction with language models through interfaces like ChatGPT, LangChain uses APIs (Application Programming Interfaces) for automated communication. These APIs act as bridges, allowing LangChain to connect to and interact with LLMs via web protocols.\n",
        "\n",
        "Each LLM provider offers its own API, and to use it, you need to obtain an access key. This key serves as authentication, enabling the service to identify and authorize your requests, execute them, and return the appropriate responses.\n",
        "\n",
        "The OpenAI API is widely used, known for its robustness and low error rate. However, it is a paid service and can become expensive depending on the model used. For learning and testing purposes, we recommend Google's API, which offers a generous amount of free requests. Check the usage limits at this [link](https://ai.google.dev/gemini-api/docs/rate-limits?hl=pt-br).\n",
        "\n",
        "To create your account and obtain access keys, visit the links below. Remember to store your keys in a safe place, as they won’t be displayed again and losing them will require generating new ones.\n",
        "\n",
        "* Gemini (Google): https://ai.google.dev/gemini-api/docs/api-key\n",
        "* OpenAI: https://platform.openai.com\n"
      ],
      "metadata": {
        "id": "QIBQHy_WOl1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's start using LangChain!\n",
        "First, we need to install the necessary dependencies to run LangChain with Google's Gemini model by executing the cell below."
      ],
      "metadata": {
        "id": "Q92VyQW-O6_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzNS1cdurk-c"
      },
      "outputs": [],
      "source": [
        "!pip install dotenv\n",
        "!pip install langchain_openai\n",
        "!pip install -qU langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to save your keys is by using a `.env` file. If you wish, create a file named `.env` and save your keys as follows:\n",
        "```\n",
        "GOOGLE_API_KEY = …\n",
        "OPENAI_API_KEY = …\n",
        "```"
      ],
      "metadata": {
        "id": "5Qx5ocIiO866"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAtJcpsIrk-f"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Inicialização do modelo de linguagem\n",
        "\n",
        "The function below simplifies the use of different models (Gemini or GPT). You can select which model you want to use and also adjust some parameters:\n",
        "\n",
        "#### Temperature:\n",
        "Temperature controls how confident the model needs to be before choosing the next word.\n",
        "\n",
        "* High Temperature (>0.7):\n",
        "  * Increases randomness. The model is more likely to pick less probable words, leading to more creative, surprising, or even chaotic responses.\n",
        "  * Good for storytelling, brainstorming, or when you want variety.\n",
        "\n",
        "* Low Temperature (<0.5):\n",
        "  * Makes output more predictable and focused. The model sticks to high-probability choices.\n",
        "  * Ideal for factual, logical, or structured tasks.\n",
        "* 0.7 is a common default for chatbots.\n",
        "* 0.0 is a common default for agents.\n",
        "\n",
        "#### Top-P (Nucleus Sampling):\n",
        "Top-p filters possible next words by cumulative probability ensuring coherent diversity. Top-p functions by dynamically filtering based on context, create a more natural and fluent output.\n",
        "\n",
        "The model sorts all possible next words by likelihood, then only considers the top ones whose total probability adds up to p (e.g., 0.9).\n",
        "  * This means the model won’t just take the top 10, but rather the top words that collectively feel “safe enough.”\n",
        "\n",
        "* 0.8–0.95 for balanced diversity and relevance.\n",
        "* Lower values (like 0.6) make the output more narrow and safe.\n",
        "\n",
        "\n",
        "You can also test these parameters on [Google AI Studio](https://aistudio.google.com/prompts/new_chat)."
      ],
      "metadata": {
        "id": "yJOTp-tMPo0N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geuQq5r5rk-h"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "def get_model_name(model_name, temperature=0):\n",
        "    if model_name == \"gemini\": # https://ai.google.dev/gemini-api/docs/rate-limits?hl=pt-br\n",
        "        if \"GOOGLE_API_KEY\" not in os.environ: # https://ai.google.dev/gemini-api/docs/api-key\n",
        "            os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
        "        llm = ChatGoogleGenerativeAI(\n",
        "            # model=\"gemini-1.5-pro\", # max 50 / dia\n",
        "            model=\"gemini-1.5-flash\", # max 1500 / dia\n",
        "            temperature=temperature,\n",
        "        )\n",
        "    elif model_name == \"openai\":\n",
        "        if \"OPENAI_API_KEY\" not in os.environ: # https://platform.openai.com\n",
        "            os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "        llm = ChatOpenAI(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=temperature,\n",
        "        )\n",
        "    return llm\n",
        "\n",
        "llm = get_model_name('gemini')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a `BaseChatModel` object named `llm`, we can test it by making our first API call using the `invoke` method with a given prompt. Try changing the prompt to observe different outputs."
      ],
      "metadata": {
        "id": "DAKtoMQBW4C-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjpnBnU2rk-h",
        "outputId": "7804b09b-abd1-4a52-d6d1-7f58012bb648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='I am doing well, thank you for asking!  How are you today?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run-e506b807-83e4-4c3c-9b0e-7c05166c60fc-0' usage_metadata={'input_tokens': 6, 'output_tokens': 17, 'total_tokens': 23, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Hello, how are you?\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to interact with the API is by setting up a conversation with the AI. The cell below creates a list called `messages`. This list is used to structure the conversation that will be sent to the language model (LLM).\n",
        "\n",
        "* Each element in the `messages` list is a tuple containing two strings:\n",
        "* The first string indicates the \"role\" of the message (\"system\" or \"human\").\n",
        "* The second string contains the content of the message.\n",
        "* `(\"system\", ...)`: Defines a system message. System messages provide instructions or context to the LLM, guiding its behavior. In this case, you're instructing the LLM to act as an assistant that translates English to French.\n",
        "* `(\"human\", ...)`: Defines a user (human) message. This is the user's input that the LLM will process. In this case, the input is the sentence \"I love programming.\""
      ],
      "metadata": {
        "id": "9-2y5E1CfWNB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTHMsTzgrk-i",
        "outputId": "0b40c0d2-da1b-4501-8078-890bbfba3ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "J'aime la programmation.\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "    ),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "]\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m6jJoDErk-k"
      },
      "source": [
        "## 1.3 Prompt Template\n",
        "\n",
        "A fundamental technique when using LangChain is the application of Prompt Templates. They allow developers to create dynamic and reusable prompts, where variables can be inserted to customize the instructions sent to LLMs during workflow execution. This not only simplifies the creation of complex prompts but also ensures consistency and makes application maintenance easier.\n",
        "\n",
        "The syntax is simple, as shown in the cell below: you define a string using triple double-quotes, and variables are inserted between curly braces. These variables represent the text that will be dynamically modified within the template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_SFfphwrk-l"
      },
      "outputs": [],
      "source": [
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ejxl9sjrk-m"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9fvoQlFrk-n",
        "outputId": "33c0b20e-16a9-469b-a4fa-72161f27709b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "prompt_template.messages[0].prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that `prompt_template` takes two input variables: `['style', 'text']`.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3TsFUyxnQ-C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x9BGHtQrk-o",
        "outputId": "06a97627-79d1-4515-a887-4b24a20bb444"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['style', 'text']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "prompt_template.messages[0].prompt.input_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s input the two variables and build the prompt that we’ll send to the LLM."
      ],
      "metadata": {
        "id": "YVrT4XkAnqla"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN6jS47Irk-p",
        "outputId": "4d0a97c1-3ffc-4440-ed09-c8b9b410e458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain_core.messages.human.HumanMessage'>\n",
            "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} response_metadata={}\n"
          ]
        }
      ],
      "source": [
        "customer_style = \"\"\"American English \\\n",
        "in a calm and respectful tone\n",
        "\"\"\"\n",
        "\n",
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse, \\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\"\n",
        "\n",
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=customer_style,\n",
        "                    text=customer_email)\n",
        "\n",
        "print(type(customer_messages))\n",
        "print(type(customer_messages[0]))\n",
        "print(customer_messages[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After using the `format_messages` method with the input variables, we build the final prompt. When inspecting `customer_messages`, we see that it consists of a list containing a single element of type `HumanMessage`. In LangChain, `HumanMessage` represents the prompts that the LLM should respond to, and not necessarily a message written by a human, as we’ll see later."
      ],
      "metadata": {
        "id": "bSsF5JzHopNe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io5qL0Eprk-q",
        "outputId": "09c2e29d-28c1-431c-e302-b463b4d6497b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm quite upset that my blender lid came off unexpectedly, resulting in smoothie splattered on my kitchen walls.  To make matters worse, the warranty doesn't appear to cover the cost of cleaning up the mess. I would appreciate your assistance with this matter.\n"
          ]
        }
      ],
      "source": [
        "# Call the LLM to translate to the style of the customer message\n",
        "customer_response = llm.invoke(customer_messages)\n",
        "print(customer_response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad7SWJ-urk-r"
      },
      "source": [
        "## 1.3 Output Parsers\n",
        "\n",
        "Now that we’ve learned how to structure the message we send to the LLM, let’s learn how to ask the LLM to return in a structured format using Output Parsers. They allow developers to define the desired format for the output, whether it's JSON, lists, objects, or any other specific structure. This is especially useful in applications that require extracting precise and structured information from LLMs, making it easier to integrate with other tools and systems.\n",
        "\n",
        "First, let's start with defining how we would like the LLM output to look like:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"gift\": False,\n",
        "  \"delivery_days\": 5,\n",
        "  \"price_value\": \"pretty affordable!\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LangChain, an effective way to define Output Parsers is by using the `pydantic` library. Pydantic is a Python library that allows the creation of robust data models using type annotations, offering data validation and automatic serialization/deserialization. For this purpose, we import:\n",
        "\n",
        "* `BaseModel`: the base class for defining structured data models.\n",
        "* `Field`: used to define individual fields within the model, allowing the addition of descriptive metadata.\n",
        "\n",
        "We can define the desired output structure by creating a class that inherits from `BaseModel`. Inside this class, each field that will make up the LLM’s output is defined as follows:\n",
        "\n",
        "```python\n",
        "    gift: bool = Field(\n",
        "        description='Describe the field for the LLM: how is it structured? How should it be filled out? What should or should not be included?'\n",
        "                            )\n",
        "\n",
        "```\n",
        "\n",
        "First, we define the name of the output field, `gift` in this example, followed by the variable type — boolean `(bool)` in this case. Inside `Field`, we provide a detailed description for the LLM, in the form of a prompt, explaining the expected content for this specific field. We can also include additional parameters to define limits and constraints for the input.\n",
        "\n",
        "You can read more about `Output Parsers` at the link below:\n",
        "\n",
        "https://python.langchain.com/docs/how_to/structured_output/\n"
      ],
      "metadata": {
        "id": "WlAGJpSIxXBh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSBXJ_C0rk-s"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "class StructuredOutput(BaseModel): # REF: https://python.langchain.com/docs/how_to/structured_output/\n",
        "    gift: bool = Field(\n",
        "        description='Was the item purchased\\\n",
        "                            as a gift for someone else? \\\n",
        "                            Answer True if yes,\\\n",
        "                            False if not or unknown.'\n",
        "                            )\n",
        "    delivery_days_schema: int = Field(\n",
        "        description=\"How many days\\\n",
        "                            did it take for the product\\\n",
        "                            to arrive? If this \\\n",
        "                            information is not found,\\\n",
        "                            output -1.\"\n",
        "                            )\n",
        "    price_value_schema: str = Field(\n",
        "        description=\"Extract any\\\n",
        "                            sentences about the value or \\\n",
        "                            price, and output them as a \\\n",
        "                            comma separated Python list.\"\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we’ve defined the LLM output format, let’s combine this format with the model."
      ],
      "metadata": {
        "id": "T48e1L6Rxvfq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUJ4K6tyrk-t",
        "outputId": "4a563766-b09d-4858-ce4d-1b1f39bdcc95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model='models/gemini-1.5-flash' google_api_key=SecretStr('**********') temperature=0.0 client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7d6b05ee6890> default_metadata=()\n",
            "first=RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7d6b05ee6890>, default_metadata=()), kwargs={'tools': [{'type': 'function', 'function': {'name': 'StructuredOutput', 'description': '', 'parameters': {'properties': {'gift': {'description': 'Was the item purchased                            as a gift for someone else?                             Answer True if yes,                            False if not or unknown.', 'type': 'boolean'}, 'delivery_days_schema': {'description': 'How many days                            did it take for the product                            to arrive? If this                             information is not found,                            output -1.', 'type': 'integer'}, 'price_value_schema': {'description': 'Extract any                            sentences about the value or                             price, and output them as a                             comma separated Python list.', 'type': 'string'}}, 'required': ['gift', 'delivery_days_schema', 'price_value_schema'], 'type': 'object'}}}], 'ls_structured_output_format': {'kwargs': {'method': 'function_calling'}, 'schema': {'type': 'function', 'function': {'name': 'StructuredOutput', 'description': '', 'parameters': {'properties': {'gift': {'description': 'Was the item purchased                            as a gift for someone else?                             Answer True if yes,                            False if not or unknown.', 'type': 'boolean'}, 'delivery_days_schema': {'description': 'How many days                            did it take for the product                            to arrive? If this                             information is not found,                            output -1.', 'type': 'integer'}, 'price_value_schema': {'description': 'Extract any                            sentences about the value or                             price, and output them as a                             comma separated Python list.', 'type': 'string'}}, 'required': ['gift', 'delivery_days_schema', 'price_value_schema'], 'type': 'object'}}}}, 'tool_choice': 'StructuredOutput'}, config={}, config_factories=[]) middle=[] last=PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.StructuredOutput'>])\n"
          ]
        }
      ],
      "source": [
        "print(llm)\n",
        "llm_with_structured_output = llm.with_structured_output(StructuredOutput)\n",
        "print(llm_with_structured_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s create a practical example. Suppose we want to extract the following data from a customer review: first, whether the purchased item was a gift; second, the delivery time mentioned in the review; and third, any comments about the product’s price.\n",
        "\n",
        "With that in mind, let’s build a sample review and the structured prompt that will be sent to the LLM to extract the desired data."
      ],
      "metadata": {
        "id": "JNzZ64ChyK4X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhosWAogrk-u"
      },
      "outputs": [],
      "source": [
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeRFujb_rk-u",
        "outputId": "4656f816-0a60-486c-8712-2ba29a2d3f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the following text, extract the following information:\n",
            "\n",
            "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\n",
            "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
            "\n",
            "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
            "\n",
            "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "review_template_2 = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product\\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
        "\n",
        "messages = prompt.format_messages(text=customer_review)\n",
        "print(messages[0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "F-N5IU51rk-v",
        "outputId": "7ed23d65-4804-4607-9988-9f99eedf1b59"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'StructuredOutput' object has no attribute 'content'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-7c2c42a62bd1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_with_structured_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    992\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m                         \u001b[0;31m# this is the current error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{type(self).__name__!r} object has no attribute {item!r}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'StructuredOutput' object has no attribute 'content'"
          ]
        }
      ],
      "source": [
        "response = llm_with_structured_output.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oops, the cell returned an error. The reason for the error is that the output no longer has the `content` field like in the previous examples, which would be the default field. Now, we have the fields we defined in the structured output instead."
      ],
      "metadata": {
        "id": "ujBfQFU10CVS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lBCUsOBrk-v",
        "outputId": "fccf732d-62b2-4a55-b887-66cbc211e541"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructuredOutput(gift=True, delivery_days_schema=2, price_value_schema=\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features\")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, the output from the LLM is of type `StructuredOutput` and contains the fields we defined.\n",
        "\n",
        "In some cases, we may want the return not as an object, but in the form of a dictionary. This can be done using the `model_dump` method, as shown in the following cell."
      ],
      "metadata": {
        "id": "wVNAvJh60Nmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuwq7PKvrk-v",
        "outputId": "86117344-6a3b-43a2-e63e-23b449ca5187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "2\n",
            "It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features\n",
            "{'gift': True, 'delivery_days_schema': 2, 'price_value_schema': \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features\"}\n"
          ]
        }
      ],
      "source": [
        "print(response.gift)\n",
        "print(response.delivery_days_schema)\n",
        "print(response.price_value_schema)\n",
        "print(response.model_dump()) # model_dump() is a pydantic method to convert the object to a dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_xalvy4rk-v"
      },
      "source": [
        "## 1.4 - Chaining\n",
        "\n",
        "In LangChain, the concept of Chaining refers to the ability to interconnect and sequence various components—including Large Language Models (LLMs), prompts, and other tools—to build complex and customized workflows.\n",
        "\n",
        "In the cell below, we’ll demonstrate the creation of a chain that processes the LLM's output and formats it into a structured format. We’ll use the `OutputParser` class to define the desired output structure and a prompt template to generate the LLM’s input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "httQuQndrk-v",
        "outputId": "7346187a-8c68-4611-d494-a6e23d7dd507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
            "gift=True delivery_days_schema=2 price_value_schema=\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features\"\n"
          ]
        }
      ],
      "source": [
        "review_template_2 = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product\\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
        "\n",
        "chain = prompt | llm_with_structured_output\n",
        "print(type(chain))\n",
        "\n",
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\"\n",
        "\n",
        "response = chain.invoke({'text':customer_review})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Task\n",
        "Develop an agent that receives customer feedback about a product. From the feedback, the chain should extract the sentiment regarding the product and the sentiment regarding the delivery time.\n",
        "\n",
        "In an LLM service, generate a few examples to test your agent. Then, develop code that evaluates each of the examples using your agent and calculates the ratio of positive sentiment for both the product and the delivery.\n"
      ],
      "metadata": {
        "id": "WWxcrf9iQ3EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the code here"
      ],
      "metadata": {
        "id": "ayarDffWSbVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tX177dUrk-w"
      },
      "source": [
        "# 2. Memory\n",
        "\n",
        "In this section, we’ll explore the concept of memory in language models and its implementation in LangChain. To begin, we’ll conduct a brief experiment. As you may have noticed, when interacting with a chat model like ChatGPT and providing your name, it remembers it throughout the conversation. However, this memory does not persist across different sessions. Let’s investigate whether the same behavior occurs in our agent by running the code in the following cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI0kOMrGrk-w",
        "outputId": "bff20f08-a84f-4203-9daf-11bb0df8723b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Jose, it's nice to meet you!  How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Hello, my name is Jose\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOnMY9oFrk-w",
        "outputId": "b535c1cf-abf8-4ed9-cb71-f2c9d568513e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I do not know your name.  I have no access to personal information about you unless you explicitly provide it to me.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What is my name?\"\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVevBFV7rk-w"
      },
      "source": [
        "By default, LLMs are stateless, meaning each interaction is treated independently, without considering the conversation history. In other words, they have no memory and no awareness that we have previously interacted with them—something we studied at the beginning of the course when exploring the transformer model. However, for many applications, especially when interacting with humans, it is essential to provide LLMs with some form of memory.\n",
        "\n",
        "In LangChain, the concept of Memory is used to manage and persist the history of interactions with LLMs. It offers various memory implementations, allowing developers to choose the one that best fits their needs. These implementations range from simple in-memory conversation storage to more complex solutions that use databases or other storage systems to persist conversation history over the long term.\n",
        "\n",
        "In other words, to give LLMs memory, we must store the history of exchanged messages and provide context to the LLM in some way.\n",
        "\n",
        "In the cell below, we define our model and create the chain. Memory simulation is done by storing the history in the `messages` key of the dictionary, which is a list representing the conversation with the AI. Initially, a \"human\" requests the translation of a sentence into French. Then, we define the AI's response using the `AIMessage` object. Finally, we run a test to verify whether the AI retains the information previously provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SspBlYdXrk-x",
        "outputId": "5be3a177-891e-4fd8-bddf-994501778f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='I said \"J\\'adore la programmation,\" which is French for \"I love programming.\"' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run-55d4205a-d79c-4d14-b2b5-601d102f938b-0' usage_metadata={'input_tokens': 40, 'output_tokens': 19, 'total_tokens': 59, 'input_token_details': {'cache_read': 0}}\n",
            "I said \"J'adore la programmation,\" which is French for \"I love programming.\"\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "response = chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(\n",
        "                content=\"Translate this sentence from English to French: I love programming.\"\n",
        "            ),\n",
        "            AIMessage(content=\"J'adore la programmation.\"),\n",
        "            HumanMessage(content=\"What did you just say?\"),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P8Th-17rk-x"
      },
      "source": [
        "## 2.1 - ConversationBufferMemory\n",
        "\n",
        "The simplest form of memory in LangChain is the `ConversationBufferMemory` class. This is a basic memory implementation that stores the conversation history, allowing the tracking of context and its provision to the LLM during response generation.\n",
        "\n",
        "In the cell below, we also import the `ConversationChain` class, which represents one of the several ways to build a conversational chain in LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7_T13_rrk-x"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, once again, let’s introduce ourselves and check if the LLM remembers our name."
      ],
      "metadata": {
        "id": "-9O5HkMEiLv0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "W5x1WY_4rk-z",
        "outputId": "e0be9004-c1cc-486c-dc2c-5a2ab1880366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Jose\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi Jose! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do.  I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, albeit somewhat literal, conversational partner.  So, what can I help you with today?  Are you interested in a specific topic, or just looking for a chat?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Jose\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "aZHqy4LBrk-1",
        "outputId": "6a4f7b84-71e0-42bf-b3f6-df19c335fafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Jose\n",
            "AI: Hi Jose! It's nice to meet you.  My name isn't really a \"name\" in the human sense, as I don't have a personal identity like you do.  I'm a large language model, trained by Google.  I don't have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, albeit somewhat literal, conversational partner.  So, what can I help you with today?  Are you interested in a specific topic, or just looking for a chat?\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Jose.  You told me that at the beginning of our conversation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we’ve seen, the conversation is stored within the `memory` parameter. We can check what’s inside the memory of our chain."
      ],
      "metadata": {
        "id": "F4hnxXHoii5A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri-Vn3k8rk-3",
        "outputId": "0a55a52d-2c86-4677-8862-6222315be3de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'Human: Hi, my name is Jose\\nAI: Hi Jose! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do.  I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, albeit somewhat literal, conversational partner.  So, what can I help you with today?  Are you interested in a specific topic, or just looking for a chat?\\nHuman: What is my name?\\nAI: Your name is Jose.  You told me that at the beginning of our conversation.'}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use the `buffer` method to print the memory in a more simplified way."
      ],
      "metadata": {
        "id": "BOwJFSvrkV1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "id": "5fWbXaPLkKcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned, an LLM has no knowledge of our past interactions, just as it naturally has no awareness of its own output. So, let’s have a bit of fun with this. Our goal now is to trick the language model by changing our name in the introduction, to make it believe it got confused.\n",
        "\n",
        "We can access the first message in memory as follows:"
      ],
      "metadata": {
        "id": "cHJEhukQlVoh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVA46ej_rk-3",
        "outputId": "6b163751-6bbf-4a3f-a612-e21a3201a604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Hi, my name is Jose' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ],
      "source": [
        "# Lets trick the AI by chaning my name on its memory\n",
        "print(memory.chat_memory.messages[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To change it, we can assign a new `HumanMessage` in place of the original one."
      ],
      "metadata": {
        "id": "0URAuZgvlsfs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ7ZQO_Zrk-4",
        "outputId": "1624c9a7-20e0-4286-bc79-2b21b9d2dd6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Hi, my name is Joao.' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ],
      "source": [
        "memory.chat_memory.messages[0] = HumanMessage(\n",
        "    content=\"Hi, my name is Joao.\"\n",
        ")\n",
        "print(memory.chat_memory.messages[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you think it will fall for it?"
      ],
      "metadata": {
        "id": "0b8l4Glblw0n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "CJOy7fiNrk-5",
        "outputId": "f4c6534e-3aff-4bb2-ccca-3f1869374fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Joao.\n",
            "AI: Hi Jose! It's nice to meet you.  My name isn't really a \"name\" in the human sense, as I don't have a personal identity like you do.  I'm a large language model, trained by Google.  I don't have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, albeit somewhat literal, conversational partner.  So, what can I help you with today?  Are you interested in a specific topic, or just looking for a chat?\n",
            "Human: What is my name?\n",
            "AI: Your name is Jose.  You told me that at the beginning of our conversation.\n",
            "Human: You are getting my name wrong. What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Oh, my apologies, Joao!  I seem to have made a mistake.  You are absolutely right; you introduced yourself as Joao, not Jose.  My apologies for the confusion.  I am still under development, and sometimes I make errors in processing information, especially when it comes to details like names.  I am learning to be more accurate.  Is there anything else I can help you with?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "conversation.predict(input=\"You are getting my name wrong. What is my name?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It’s also possible to completely rewrite the memory, as shown in the cell below."
      ],
      "metadata": {
        "id": "8mrUcZ9kmG0l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JVSc1lqrk-5",
        "outputId": "aa4e5b6f-662d-413d-93f3-82137b83ac8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Hi\n",
            "AI: What's up\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\"}"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can also save new messages to the memory\n",
        "memory = ConversationBufferMemory()\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "print(memory.buffer)\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktYY2e69rk_A"
      },
      "source": [
        "## 2.2 - ConversationBufferWindowMemory\n",
        "\n",
        "The problem with storing the entire conversation using `ConversationBufferMemory` is that it can lead to exceeding the token limit of the API, resulting in errors, and the loss of relevant information as the LLM gets lost in an extensive history. To mitigate these challenges, LangChain offers more sophisticated memory management alternatives.\n",
        "\n",
        "The simplest of these is the `ConversationBufferWindowMemory` class, designed to keep track of the last “k” interactions in a conversation. When the number of messages exceeds the configured maximum limit, the oldest messages are discarded, ensuring that recent context is preserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG0V4Lv6rk_B",
        "outputId": "8d85e565-acc3-419c-ac5d-bf06e8566ae8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diego\\AppData\\Local\\Temp\\ipykernel_17540\\3713523315.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "\n",
        "memory.load_memory_variables({})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example in the cell below, we’ll keep the window very small `(k=1)`, meaning our memory will store only the last interaction with the LLM."
      ],
      "metadata": {
        "id": "8TdIWnyjq8xV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KvvKNnRrk_C",
        "outputId": "0a2f1cd0-b316-4fe0-d7fd-1ee4d88e8e75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello Jose! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do.  I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, albeit somewhat literal, conversational partner.  So, what can I help you with today?  Are you interested in a specific topic, or just looking for a chat?'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")\n",
        "conversation.predict(input=\"Hello, my name is Jose\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqdB5Xh3rk_E",
        "outputId": "ff302fbe-1178-4d8c-9bd4-a6f005e1089d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'As a large language model, I don\\'t experience emotions or have a sense of well-being in the same way humans do.  I don\\'t \"feel\" happy, sad, tired, or any other emotion.  However, my systems are currently operational and functioning as intended.  My processors are running smoothly, my access to information is unimpeded, and I\\'m ready to assist you with any questions or tasks you might have.  Is there anything specific you\\'d like to discuss or ask me about?'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"How are you doing?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooGA4fSQrk_F",
        "outputId": "142c09d9-b531-4e6f-d5db-e35dd427d695"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"AI: I do not have access to your personally identifiable information, including your name.  My purpose is to provide helpful and informative responses based on the data I've been trained on, but that data does not include private details about individuals.  To protect user privacy, I am designed to not retain or access such information.  Is there anything else I can help you with?\""
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can observe, the LLM once again cannot recall our name, because the provided context no longer includes our initial interaction where we introduced ourselves."
      ],
      "metadata": {
        "id": "lAGnXQSqrb2R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YYfg7N3rk_F"
      },
      "source": [
        "## 2.3 - ConversationTokenBufferMemory\n",
        "\n",
        "Another efficient way to manage memory with size control is the `ConversationTokenBufferMemory` class. This memory implementation retains only the most recent messages within a specified token limit. It’s important to remember that one token corresponds to approximately 4 characters of English text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv7OzdcCrk_F",
        "outputId": "81e3f4e0-5f37-4f42-ff04-b8078aba5abc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=20)\n",
        "\n",
        "memory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSkffMomrk_F"
      },
      "source": [
        "## 2.4 - ConversationSummaryMemory\n",
        "\n",
        "The problem with the previous methods is that they handle memory overflow by simply removing information once a limit is reached. As we've seen earlier, this approach can lead to the loss of important information.\n",
        "\n",
        "A more efficient way to handle excess information, as already discussed in this course, is to continuously summarize the conversation history. The `ConversationSummaryMemory` class implements exactly this solution, updating the summary after each interaction.\n",
        "\n",
        "With every turn in the conversation, the summary is updated, providing a condensed view of the history. This summary can be used as context for the model in future calls.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6K5Ku5krk_G",
        "outputId": "287196b3-5745-4eb5-9c8f-3f359d99b3d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\diego\\AppData\\Local\\Temp\\ipykernel_17540\\29023286.py:12: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'history': 'System: The human greets the AI, and after brief pleasantries, asks what is on the schedule for the day.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "# create a long string\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJkv8uqbrk_G",
        "outputId": "6659dc28-770f-478d-c2e9-33a1c7810f56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human greets the AI, and after brief pleasantries, asks what is on the schedule for the day.\n",
            "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
            "Human: What would be a good demo to show?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"That's a great question!  Since your customer is driving over an hour to learn about the latest in AI, I'd recommend focusing on a demo that showcases the practical applications of LLMs and is relatively easy to understand, even for someone without a deep technical background.  Here are a few options, ranked in order of my recommendation:\\n\\n1. **A customized question-answering system using LangChain:** This directly relates to your LangChain project and highlights its power. You could pre-load it with information relevant to your customer's industry or needs.  The demo could involve asking it a few insightful questions, demonstrating its ability to synthesize information and provide concise, accurate answers.  This is impactful because it shows a real-world application of the technology.\\n\\n2. **A simple text summarization demo:**  This is visually appealing and easy to grasp. You could feed it a longer article or document relevant to their business and show how quickly and accurately it summarizes the key points.  This demonstrates the efficiency LLMs offer.\\n\\n3. **A creative text generation demo (with caveats):**  You could show it generating different creative text formats, like poems, scripts, or marketing copy. However, be cautious here.  The output might not always be perfect, and you need to manage expectations.  Frame it as showcasing the *potential* of LLMs for creative tasks, rather than a perfectly polished product.  This is less impactful than the first two options unless the customer is specifically interested in creative applications.\\n\\n**Important Considerations for *all* demos:**\\n\\n* **Keep it concise:** Aim for a 5-10 minute demo, max.  Respect your customer's time.\\n* **Focus on the value proposition:**  Clearly show how the LLM technology can benefit *them*.  Connect the demo to their specific needs and pain points.\\n* **Have a backup plan:**  Technology can be unpredictable.  Have a simple alternative ready in case something goes wrong.\\n* **Prepare talking points:**  Don't just show the demo; explain what's happening, why it's impressive, and how it relates to their business.\\n\\nI recommend option 1 (the customized Q&A system) most strongly because it directly ties into your work and showcases LangChain's capabilities in a practical, business-relevant way.  Remember to tailor the demo to your customer's specific industry and interests for maximum impact.\""
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "conversation.predict(input=\"What would be a good demo to show?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qr2wDCPrk_G",
        "outputId": "98d4ab25-4c84-48c9-cca7-4e29ffd037b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': \"System: The human greets the AI and inquires about the day's schedule. The AI outlines a busy day including a product team meeting, work on a LangChain project, and a lunch meeting with a customer.  The AI suggests several LLM demos for the customer meeting, prioritizing a customized question-answering system using LangChain due to its relevance and ease of understanding, followed by text summarization, and finally, creative text generation (with caveats about potential imperfections).  The AI emphasizes keeping the demo concise, focusing on the customer's needs, having a backup plan, and preparing relevant talking points to maximize impact.\"}"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To deepen your understanding of memory in LangChain, refer to the first link below. Additionally, as LangChain is constantly evolving and updating, a new form of memory has been introduced. This new approach requires a higher level of programming proficiency. If you're interested in exploring it, the second link provides detailed information.\n",
        "\n",
        "Memory in LangChain:\n",
        "\n",
        "* ver 0.2: https://python.langchain.com/api_reference/langchain/memory.html\n",
        "* ver 0.3: https://python.langchain.com/docs/versions/migrating_memory/\n"
      ],
      "metadata": {
        "id": "g21IdQSPyx5-"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchainV2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}