{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFLym7rW2QCN"
      },
      "source": [
        "# Section 4. Agents\n",
        "\n",
        "\"An AI agent is a system that uses an LLM to decide the control flow of an application.\"\n",
        "‚Äî Harrison Chase, co-founder of LangChain\n",
        "\n",
        "In other words, an agent is a system that uses a language model to determine the control flow of an application.\n",
        "\n",
        "In the context of LangChain, agents are components that use language models to make decisions about which actions to take and in what order. Unlike chains, where the sequence of actions is predefined, agents use the language model as a reasoning engine to dynamically decide the actions to be taken.\n",
        "\n",
        "LangChain agents are designed to interact with various tools, process inputs, and generate meaningful outputs. Instead of providing static responses, agents follow a looped structure to perform reasoning and iterative steps until the desired output is reached.\n",
        "\n",
        "This dynamic approach allows agents to handle complex queries and adapt to different scenarios, making them essential components for building more intelligent and adaptable AI applications."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install dotenv\n",
        "# !pip install langchain_openai\n",
        "# !pip install -qU langchain-google-genai\n",
        "# !pip install -U wikipedia\n",
        "# !pip install -U numexpr\n",
        "# !pip install langchain_community\n",
        "# !pip install langgraph\n",
        "# !pip install DateTime"
      ],
      "metadata": {
        "id": "Q8PL0AhWEuOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z-MibVt2QCO"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbGhenAL2QCP"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "def get_model_name(model_name, temperature=0):\n",
        "    if model_name == \"gemini\": # https://ai.google.dev/gemini-api/docs/rate-limits?hl=pt-br\n",
        "        if \"GOOGLE_API_KEY\" not in os.environ: # https://ai.google.dev/gemini-api/docs/api-key\n",
        "            os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
        "        llm = ChatGoogleGenerativeAI(\n",
        "            # model=\"gemini-1.5-pro\", # max 50 / dia\n",
        "            model=\"gemini-1.5-flash\", # max 1500 / dia\n",
        "            temperature=temperature,\n",
        "        )\n",
        "    elif model_name == \"openai\":\n",
        "        if \"OPENAI_API_KEY\" not in os.environ: # https://platform.openai.com\n",
        "            os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "        llm = ChatOpenAI(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=temperature,\n",
        "        )\n",
        "    return llm\n",
        "\n",
        "llm = get_model_name('gemini')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYNaDG9m2QCP"
      },
      "source": [
        "## 4.1 Tools\n",
        "\n",
        "Tools are external functions that agents can use to perform specific tasks. They work like ‚Äúskills‚Äù that expand what the agent is capable of doing. A well-defined agent is aware of which tools are available and understands how to use them‚Äîthat is, what inputs and outputs are expected for the function defining the tool.\n",
        "\n",
        "LangChain offers many built-in tools that can simplify the implementation of agent pipelines. In this lesson, we‚Äôll look at some of them and then learn how to create a new tool.\n",
        "\n",
        "Built-in LangChain tools: https://python.langchain.com/docs/integrations/tools/\n",
        "\n",
        "Naturally, a Large Language Model (LLM) does not, by itself, have the knowledge needed to call functions or interact directly with the external environment. For this, it‚Äôs essential that the LLM‚Äôs output is well structured and follows a defined format that can be interpreted by the system.\n",
        "\n",
        "To enable an LLM to act as an agent, we use a prompting technique known as ReAct (Reasoning + Acting), which we studied earlier. With this method, the LLM is able to reason through the task and, based on that, decide what action to take‚Äîsuch as calling a specific tool.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in the cell below, we‚Äôll begin exploring how to define tools and create a ReAct-style agent. This type of agent is capable of intelligently selecting and using tools before returning a response to us.\n",
        "\n",
        "First, we import two important libraries:\n",
        "* `load_tools`, which loads built-in LangChain tools‚Äîin this case, a calculator and a tool that accesses Wikipedia.\n",
        "\n",
        "* `create_react_agent`, which creates a ReAct-style agent using LangGraph as the executor.\n",
        "\n",
        "Later on, we‚Äôll study LangGraph in more depth. But in summary, it allows you to design an execution graph through which the agent can navigate while reasoning, until it decides it has found the answer and ends execution."
      ],
      "metadata": {
        "id": "QqHhLeXSG9nY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52VUUSyD2QCQ"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)\n",
        "\n",
        "langgraph_agent_executor = create_react_agent(llm, tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let‚Äôs test our agent with a math question.\n",
        "\n",
        "In the last cell, all the actions taken by the LLM to answer the prompt are available in the `\"messages\"` key of the return.\n",
        "\n",
        "By analyzing these messages, we see that the first is our original prompt sent to the LLM. Then, it makes a `function_call` with the name `Calculator`. Right after that, the `Calculator` tool responds with the result of the calculation. Finally, the LLM uses this information to formulate the final answer.\n",
        "\n",
        "Cool ‚Äî we were able to see the sequence of actions taken by the LLM. However, the format isn‚Äôt very clear, and the visualization can be a bit confusing, which makes debugging or understanding what happened more difficult.\n",
        "\n",
        "Wouldn‚Äôt it be great if there was some kind of IDE to track all of this in a more visual way?"
      ],
      "metadata": {
        "id": "GMvaqJFJIqmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the 25 percent of 300?\"\n",
        "response = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]})\n",
        "\n",
        "# Show response\n",
        "for msg in response[\"messages\"]:\n",
        "    print(msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z5bdc1MIj1u",
        "outputId": "6b665d8d-3d34-4171-df10-92cdafca0c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='What is the 25 percent of 300?' additional_kwargs={} response_metadata={} id='c5ce7219-c96e-4494-a04a-44cafa69c8d5'\n",
            "content='' additional_kwargs={'function_call': {'name': 'Calculator', 'arguments': '{\"__arg1\": \"25 percent of 300\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run-8a5ec94e-3c53-4fcc-bdcd-7967e61046b1-0' tool_calls=[{'name': 'Calculator', 'args': {'__arg1': '25 percent of 300'}, 'id': '6f494fe1-f15e-465a-ab50-ffacc9ac23aa', 'type': 'tool_call'}] usage_metadata={'input_tokens': 77, 'output_tokens': 12, 'total_tokens': 89, 'input_token_details': {'cache_read': 0}}\n",
            "content='Answer: 75.0' name='Calculator' id='e20c2faf-0c27-4f78-84b9-29120a7fda1f' tool_call_id='6f494fe1-f15e-465a-ab50-ffacc9ac23aa'\n",
            "content='25 percent of 300 is 75.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run-3b77dde8-5e0c-4951-8c72-42294f990466-0' usage_metadata={'input_tokens': 98, 'output_tokens': 14, 'total_tokens': 112, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 LangSmith\n",
        "\n",
        "LangSmith is a platform created by the LangChain team with the goal of helping developers debug, monitor, evaluate, and improve applications based on language models (LLMs).\n",
        "\n",
        "When working with complex agents and chains, it can be difficult to understand exactly what‚Äôs happening behind the model‚Äôs decisions. That‚Äôs where LangSmith comes in‚Äîas a sort of ‚Äúlab‚Äù for experimentation and observability for LLMs.\n",
        "\n",
        "Among its features, the most commonly used is tracing, which allows you to view in detail how a chain or agent executed a task‚Äîfrom the user input to tool calls and the final generated responses.\n",
        "\n",
        "To use LangSmith, first create an account on the official site: [LangSmith](https://smith.langchain.com/). Then, generate a new API key from your account.\n",
        "\n",
        "Activating LangSmith is very simple: just set the environment variable `LANGSMITH_TRACING` to `true`. With that, executions from LangChain and LangGraph will automatically be synced to LangSmith, in the project specified by the `LANGSMITH_PROJECT` variable."
      ],
      "metadata": {
        "id": "z249Zk5AGod5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
        "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
        "\n",
        "os.environ['LANGSMITH_TRACING'] = 'true'\n",
        "os.environ['LANGSMITH_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"insper_llm\""
      ],
      "metadata": {
        "id": "P2_b6Jx5GrLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqszT7NA2QCR",
        "outputId": "644a9f33-f33f-4cea-d69e-8df4447dc894"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Tom M. Mitchell is an American computer scientist and the Founders University Professor at Carnegie Mellon University (CMU)what book did he write?' additional_kwargs={} response_metadata={} id='d3ba48a2-5462-4c2e-906a-4fb64f3461f6'\n",
            "content='I am sorry, I do not have access to external websites or specific information about authors and their works.  Therefore, I cannot tell you what book Tom M. Mitchell wrote.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run-363ecab1-e0d9-49ef-bba7-97b8c9ae9d74-0' usage_metadata={'input_tokens': 88, 'output_tokens': 37, 'total_tokens': 125, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ],
      "source": [
        "query = \"Tom M. Mitchell is an American computer scientist \\\n",
        "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
        "what book did he write?\"\n",
        "response = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]})\n",
        "\n",
        "# Show response\n",
        "for msg in response[\"messages\"]:\n",
        "    print(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRNIFM-F2QCR"
      },
      "source": [
        "## 4.3 Python REPL Agent\n",
        "\n",
        "The Python REPL Agent is a type of agent that has the ability to execute Python code in real time. It‚Äôs extremely useful when you want the LLM to handle tasks involving more complex logic, precise mathematical calculations, list manipulation, dates, or any other type of detailed programming.\n",
        "\n",
        "REPL stands for:\n",
        "\n",
        "Read‚ÄìEval‚ÄìPrint Loop ‚Äî in other words, an environment where the code is read, executed, and the result is returned immediately.\n",
        "\n",
        "The execution environment is controlled, but it's still important to remember: running arbitrary code generated by an LLM requires caution, especially in production environments, so always be careful when using REPL agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuzF2HM12QCR",
        "outputId": "da5b81e6-02f1-4737-e273-e86a0d7b3918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to sort the list of lists based on the last name and then the first name.  I'll use the `sorted()` function with a custom key.\n",
            "Action: Python_REPL\n",
            "Action Input: \n",
            "```python\n",
            "customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\n",
            "sorted_customers = sorted(customers, key=lambda x: (x[0], x[1]))\n",
            "print(sorted_customers)\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m[['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Harrison', 'Chase'], ['Jen', 'Ayai'], ['Lang', 'Chain'], ['Trance', 'Former']]\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "Final Answer: [['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Harrison', 'Chase'], ['Jen', 'Ayai'], ['Lang', 'Chain'], ['Trance', 'Former']]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
              " 'output': \"[['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Harrison', 'Chase'], ['Jen', 'Ayai'], ['Lang', 'Chain'], ['Trance', 'Former']]\"}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "\n",
        "agent = create_python_agent(\n",
        "    llm,\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "customer_list = [[\"Harrison\", \"Chase\"],\n",
        "                 [\"Lang\", \"Chain\"],\n",
        "                 [\"Dolly\", \"Too\"],\n",
        "                 [\"Elle\", \"Elem\"],\n",
        "                 [\"Geoff\",\"Fusion\"],\n",
        "                 [\"Trance\",\"Former\"],\n",
        "                 [\"Jen\",\"Ayai\"]\n",
        "                ]\n",
        "\n",
        "agent.invoke(f\"\"\"Sort these customers by \\\n",
        "last name and then first name \\\n",
        "and print the output: {customer_list}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the execution, we can see that the agent first generated Python code to solve the problem. Then, that code was executed in the REPL environment, and the resulting output was returned to us, the user."
      ],
      "metadata": {
        "id": "qRQJpt_5O3qZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM8K8k9S2QCR"
      },
      "source": [
        "### 4.3.1 PythonREPL + create_react_agent\n",
        "\n",
        "Now, let‚Äôs repeat the same process using create_react_agent from LangGraph.\n",
        "This time, we start by defining the PythonREPL object and then associating it with a Tool:\n",
        "\n",
        "```\n",
        "# Tool\n",
        "python_repl = PythonREPL()\n",
        "print(python_repl.run(\"print(1+1)\"))\n",
        "tools = [Tool(\n",
        "    name=\"python_repl\",\n",
        "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
        "    func=python_repl.run,\n",
        ")]\n",
        "```\n",
        "\n",
        "After that, we define the system prompt and create the agent.\n",
        "\n",
        "It's worth noting that, due to some limitations in ReAct-style prompting, this cell may not always work correctly with Gemini. This highlights one of the advantages of the OpenAI API, which is currently more optimized to handle structured outputs and the ReAct method more consistently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX9TJR5N2QCS",
        "outputId": "00669fa9-3aca-4236-e893-397b67a291fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "\n",
            "content=\"Use the Python tool to sort this list of customers by last name, then first name, and print the result:\\n[['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\" additional_kwargs={} response_metadata={} id='7033a92a-8eb0-4e37-b6d1-b3bbc2e46f67'\n",
            "content='' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'MALFORMED_FUNCTION_CALL', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run-4457557e-cdff-47a0-b91f-f3d19f16be22-0' usage_metadata={'input_tokens': 131, 'output_tokens': 0, 'total_tokens': 131, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain_core.tools import Tool\n",
        "import langchain\n",
        "\n",
        "llm_openai = get_model_name('openai', temperature=0.1)\n",
        "\n",
        "# Tool\n",
        "python_repl = PythonREPL()\n",
        "print(python_repl.run(\"print(1+1)\"))\n",
        "tools = [Tool(\n",
        "    name=\"python_repl\",\n",
        "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
        "    func=python_repl.run,\n",
        ")]\n",
        "\n",
        "# System message\n",
        "system_message = \"You are a helpful assistant. Use the tools to answer user questions.\"\n",
        "\n",
        "# Agent executor\n",
        "langgraph_agent_executor = create_react_agent(llm_openai, tools, prompt=system_message)\n",
        "\n",
        "# Data\n",
        "customer_list = [[\"Harrison\", \"Chase\"],\n",
        "                 [\"Lang\", \"Chain\"],\n",
        "                 [\"Dolly\", \"Too\"],\n",
        "                 [\"Elle\", \"Elem\"],\n",
        "                 [\"Geoff\",\"Fusion\"],\n",
        "                 [\"Trance\",\"Former\"],\n",
        "                 [\"Jen\",\"Ayai\"]]\n",
        "\n",
        "# Prompt\n",
        "message = f\"\"\"Use the Python tool to sort this list of customers by last name, then first name, and print the result:\\n{customer_list}\"\"\"\n",
        "\n",
        "# Run\n",
        "response = langgraph_agent_executor.invoke({\n",
        "    \"messages\": [HumanMessage(content=message)]\n",
        "})\n",
        "\n",
        "# Show response\n",
        "for msg in response[\"messages\"]:\n",
        "    print(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rdFrNpg2QCS"
      },
      "source": [
        "Besides LangSmith, another way to view an agent‚Äôs execution in detail is as shown in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPTG8F1N2QCS",
        "outputId": "f6acc161-520e-441b-d069-46878807d8b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
            "  \"agent_scratchpad\": \"\",\n",
            "  \"stop\": [\n",
            "    \"\\nObservation:\",\n",
            "    \"\\n\\tObservation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython_REPL - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python_REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatGoogleGenerativeAI] [1.20s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"Thought: I need to sort the list of lists based on the last name and then the first name.  I'll use the `sorted()` function with a custom key.\\nAction: Python_REPL\\nAction Input: \\n```python\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[0], x[1]))\\nprint(sorted_customers)\\n```\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"model_name\": \"gemini-1.5-flash\",\n",
            "          \"safety_ratings\": []\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"Thought: I need to sort the list of lists based on the last name and then the first name.  I'll use the `sorted()` function with a custom key.\\nAction: Python_REPL\\nAction Input: \\n```python\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[0], x[1]))\\nprint(sorted_customers)\\n```\",\n",
            "            \"response_metadata\": {\n",
            "              \"prompt_feedback\": {\n",
            "                \"block_reason\": 0,\n",
            "                \"safety_ratings\": []\n",
            "              },\n",
            "              \"finish_reason\": \"STOP\",\n",
            "              \"model_name\": \"gemini-1.5-flash\",\n",
            "              \"safety_ratings\": []\n",
            "            },\n",
            "            \"type\": \"ai\",\n",
            "            \"id\": \"run-5dd12be6-6191-4ad7-8cf7-7bf1833ab35b-0\",\n",
            "            \"usage_metadata\": {\n",
            "              \"input_tokens\": 327,\n",
            "              \"output_tokens\": 130,\n",
            "              \"total_tokens\": 457,\n",
            "              \"input_token_details\": {\n",
            "                \"cache_read\": 0\n",
            "              }\n",
            "            },\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"prompt_feedback\": {\n",
            "      \"block_reason\": 0,\n",
            "      \"safety_ratings\": []\n",
            "    }\n",
            "  },\n",
            "  \"run\": null,\n",
            "  \"type\": \"LLMResult\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [1.20s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"Thought: I need to sort the list of lists based on the last name and then the first name.  I'll use the `sorted()` function with a custom key.\\nAction: Python_REPL\\nAction Input: \\n```python\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[0], x[1]))\\nprint(sorted_customers)\\n```\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:Python_REPL] Entering Tool run with input:\n",
            "\u001b[0m\"```python\n",
            "customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\n",
            "sorted_customers = sorted(customers, key=lambda x: (x[0], x[1]))\n",
            "print(sorted_customers)\n",
            "```\"\n",
            "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:Python_REPL] [2ms] Exiting Tool run with output:\n",
            "\u001b[0m\"[['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Harrison', 'Chase'], ['Jen', 'Ayai'], ['Lang', 'Chain'], ['Trance', 'Former']]\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
            "  \"agent_scratchpad\": \"Thought: I need to sort the list of lists based on the last name and then the first name.  I'll use the `sorted()` function with a custom key.\\nAction: Python_REPL\\nAction Input: \\n```python\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[0], x[1]))\\nprint(sorted_customers)\\n```\\nObservation: [['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Harrison', 'Chase'], ['Jen', 'Ayai'], ['Lang', 'Chain'], ['Trance', 'Former']]\\n\\nThought:\",\n",
            "  \"stop\": [\n",
            "    \"\\nObservation:\",\n",
            "    \"\\n\\tObservation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatGoogleGenerativeAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython_REPL - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python_REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:Thought: I need to sort the list of lists based on the last name and then the first name.  I'll use the `sorted()` function with a custom key.\\nAction: Python_REPL\\nAction Input: \\n```python\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[0], x[1]))\\nprint(sorted_customers)\\n```\\nObservation: [['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Harrison', 'Chase'], ['Jen', 'Ayai'], ['Lang', 'Chain'], ['Trance', 'Former']]\\n\\nThought:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatGoogleGenerativeAI] [839ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"Thought: I now know the final answer\\nFinal Answer: [['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Harrison', 'Chase'], ['Jen', 'Ayai'], ['Lang', 'Chain'], ['Trance', 'Former']]\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"model_name\": \"gemini-1.5-flash\",\n",
            "          \"safety_ratings\": []\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"Thought: I now know the final answer\\nFinal Answer: [['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Harrison', 'Chase'], ['Jen', 'Ayai'], ['Lang', 'Chain'], ['Trance', 'Former']]\",\n",
            "            \"response_metadata\": {\n",
            "              \"prompt_feedback\": {\n",
            "                \"block_reason\": 0,\n",
            "                \"safety_ratings\": []\n",
            "              },\n",
            "              \"finish_reason\": \"STOP\",\n",
            "              \"model_name\": \"gemini-1.5-flash\",\n",
            "              \"safety_ratings\": []\n",
            "            },\n",
            "            \"type\": \"ai\",\n",
            "            \"id\": \"run-d88be615-2c16-4ac9-90fa-005a18d00756-0\",\n",
            "            \"usage_metadata\": {\n",
            "              \"input_tokens\": 507,\n",
            "              \"output_tokens\": 57,\n",
            "              \"total_tokens\": 564,\n",
            "              \"input_token_details\": {\n",
            "                \"cache_read\": 0\n",
            "              }\n",
            "            },\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"prompt_feedback\": {\n",
            "      \"block_reason\": 0,\n",
            "      \"safety_ratings\": []\n",
            "    }\n",
            "  },\n",
            "  \"run\": null,\n",
            "  \"type\": \"LLMResult\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [840ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"Thought: I now know the final answer\\nFinal Answer: [['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Harrison', 'Chase'], ['Jen', 'Ayai'], ['Lang', 'Chain'], ['Trance', 'Former']]\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [2.05s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"[['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Harrison', 'Chase'], ['Jen', 'Ayai'], ['Lang', 'Chain'], ['Trance', 'Former']]\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "langchain.debug=True\n",
        "agent.run(f\"\"\"Sort these customers by \\\n",
        "last name and then first name \\\n",
        "and print the output: {customer_list}\"\"\")\n",
        "langchain.debug=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKz57RYq2QCS"
      },
      "source": [
        "## 4.4 Pandas Dataframe Agent\n",
        "\n",
        "Another type of agent used in many applications is the [Pandas Dataframe Agent](https://python.langchain.com/docs/integrations/tools/pandas/). This agent specializes in generating code to analyze tables (dataframes) that are directly attached to the agent. It uses Python‚Äôs pandas library to manipulate and interpret the data, and then executes that code through the Python REPL, delivering the requested analysis to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DBqESf42QCS",
        "outputId": "71863cf7-5636-48d1-f421-2753eecdf9b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': 'len(df)'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m891\u001b[0m\u001b[32;1m\u001b[1;3mThere are 891 rows in the dataframe.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'how many rows are there?',\n",
              " 'output': 'There are 891 rows in the dataframe.'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv\"\n",
        ")\n",
        "\n",
        "agent = create_pandas_dataframe_agent(\n",
        "    llm,\n",
        "    df,\n",
        "    # agent_type=AgentType.OPENAI_FUNCTIONS, # only for OpenAI models\n",
        "    verbose=True,\n",
        "    allow_dangerous_code=True, # for security - will only work if True\n",
        "    )\n",
        "agent.invoke(\"how many rows are there?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2_t7POY2QCS"
      },
      "source": [
        "## 4.5 How To Create Tools\n",
        "\n",
        "LangChain already provides a wide range of ready-to-use tools‚Äîexternal APIs, databases, browsers, etc. That‚Äôs why it‚Äôs highly recommended to explore the full list of available tools, along with usage examples, directly in the official LangChain documentation: [link](https://python.langchain.com/docs/integrations/tools/).\n",
        "\n",
        "In many situations, we need to create custom tools to meet specific needs in our agent applications. The simplest way to do this is by using the `@tool` decorator.\n",
        "\n",
        "By default, the decorator uses the function name as the tool name, but this can be overridden by passing a string as the first argument. In addition, it uses the function‚Äôs docstring as the tool‚Äôs description ‚Äî so it‚Äôs mandatory to provide an appropriate docstring.\n",
        "\n",
        "In the cell below, we define a simple function called `time`, which returns the current time. As mentioned, when using the `@tool` decorator, it‚Äôs important to define both the docstring and input/output type annotations, as this information is automatically passed to the agent so it knows what the tool does and how to use it.\n",
        "\n",
        "Finally, we print the tool‚Äôs name, parameters, and description, showing exactly how they will be presented to the agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1d9jg_u2QCT"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import tool\n",
        "from datetime import date\n",
        "\n",
        "@tool\n",
        "def time(text: str) -> str:\n",
        "    \"\"\"Returns todays date, use this for any \\\n",
        "    questions related to knowing todays date. \\\n",
        "    The input should always be an empty string, \\\n",
        "    and this function will always return todays \\\n",
        "    date - any date mathmatics should occur \\\n",
        "    outside this function.\"\"\"\n",
        "    return str(date.today())\n",
        "\n",
        "\n",
        "print(time.name)\n",
        "print(time.description)\n",
        "print(time.args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbLvMMxd2QCT",
        "outputId": "0f3a50c0-38fe-41b7-bd17-5a26d812d144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='whats the date today?' additional_kwargs={} response_metadata={} id='2d2eaa0a-41b6-46a3-817c-30d0d672d00e'\n",
            "content='Bro...' additional_kwargs={'function_call': {'name': 'time', 'arguments': '{\"text\": \"\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run-62263a07-bbee-41de-af9c-9dd94f86aa25-0' tool_calls=[{'name': 'time', 'args': {'text': ''}, 'id': '7d07191b-c475-4aca-bffa-ae3939ed0a83', 'type': 'tool_call'}] usage_metadata={'input_tokens': 127, 'output_tokens': 4, 'total_tokens': 131, 'input_token_details': {'cache_read': 0}}\n",
            "content='2025-03-29' name='time' id='69c07920-ae63-474d-a842-b3106db37e6a' tool_call_id='7d07191b-c475-4aca-bffa-ae3939ed0a83'\n",
            "content=\"Bro...Today's date is 2025-03-29. got it?\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run-3708d0cb-8191-449e-b988-a328aa972fe2-0' usage_metadata={'input_tokens': 141, 'output_tokens': 23, 'total_tokens': 164, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant, always start a answer with 'Bro...' and end with 'got it?' \"),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "## or simply use the prompt template as a string\n",
        "# system_message = \"You are a helpful assistant, always start a answer with 'Bro...' and end with 'got it?' \"\n",
        "\n",
        "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)\n",
        "tools = tools + [time]\n",
        "langgraph_agent_executor = create_react_agent(llm, tools, prompt=prompt)\n",
        "\n",
        "query = \"whats the date today?\"\n",
        "response = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]})\n",
        "\n",
        "# Show response\n",
        "for msg in response[\"messages\"]:\n",
        "    print(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5.1 More On Tool Creation\n",
        "\n",
        "There are two other ways to define a custom tool‚Äîcheck out the LangChain tutorial to learn more:\n",
        "\n",
        "[How to create tools | ü¶úÔ∏èüîó LangChain](https://python.langchain.com/docs/how_to/custom_tools/)"
      ],
      "metadata": {
        "id": "koQVmHTnNmJA"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchainV2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}